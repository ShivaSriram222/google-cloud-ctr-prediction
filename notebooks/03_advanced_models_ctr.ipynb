{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc52e22b-11fe-4718-a984-788b19438f82",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time, pandas as pd\n",
    "import pyarrow.dataset as ds\n",
    "from pyarrow.fs import GcsFileSystem, FileSelector\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, log_loss, accuracy_score, average_precision_score\n",
    "import xgboost as xgb\n",
    "import joblib\n",
    "\n",
    "BUCKET_DIR = \"avazu-ctr/processed/fe_v2_parquet\" \n",
    "COLUMNS = [\n",
    "    \"id\",\"click\",\n",
    "    \"hour_of_day\",\"day_of_week\",\"is_weekend\",\n",
    "    \"site_freq\",\"app_freq\",\"device_freq\",\n",
    "    \"te_site_id\",\"te_app_id\",\"te_device_model\",\"te_C14\",\n",
    "    \"hash_sitecat_devtype\",\"hash_appcat_devtype\",\"hash_c14_c17\",\n",
    "]\n",
    "N_FILES = 30 # start small; bump to 120/240 later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a2f8790-d46e-4b6e-9f4a-c06e815a2108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parquet parts found: 1500\n",
      "Loaded: (15374036, 15) in 11.4s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>click</th>\n",
       "      <th>hour_of_day</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>is_weekend</th>\n",
       "      <th>site_freq</th>\n",
       "      <th>app_freq</th>\n",
       "      <th>device_freq</th>\n",
       "      <th>te_site_id</th>\n",
       "      <th>te_app_id</th>\n",
       "      <th>te_device_model</th>\n",
       "      <th>te_C14</th>\n",
       "      <th>hash_sitecat_devtype</th>\n",
       "      <th>hash_appcat_devtype</th>\n",
       "      <th>hash_c14_c17</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.43198015250962e+19</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>14596137</td>\n",
       "      <td>21579</td>\n",
       "      <td>33358308</td>\n",
       "      <td>0.122886</td>\n",
       "      <td>0.001092</td>\n",
       "      <td>0.107136</td>\n",
       "      <td>0.002479</td>\n",
       "      <td>-4130613151412272962</td>\n",
       "      <td>518379290858953980</td>\n",
       "      <td>4808654653650998822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.4342335790673269e+19</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>14596137</td>\n",
       "      <td>21579</td>\n",
       "      <td>33358308</td>\n",
       "      <td>0.122886</td>\n",
       "      <td>0.001092</td>\n",
       "      <td>0.109834</td>\n",
       "      <td>0.002479</td>\n",
       "      <td>-4130613151412272962</td>\n",
       "      <td>518379290858953980</td>\n",
       "      <td>4808654653650998822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.5949798049261076e+19</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>14596137</td>\n",
       "      <td>21579</td>\n",
       "      <td>33358308</td>\n",
       "      <td>0.122886</td>\n",
       "      <td>0.001092</td>\n",
       "      <td>0.109834</td>\n",
       "      <td>0.002479</td>\n",
       "      <td>-4130613151412272962</td>\n",
       "      <td>518379290858953980</td>\n",
       "      <td>4808654653650998822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6.2652196368133274e+18</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>14596137</td>\n",
       "      <td>21579</td>\n",
       "      <td>33358308</td>\n",
       "      <td>0.122886</td>\n",
       "      <td>0.001092</td>\n",
       "      <td>0.128226</td>\n",
       "      <td>0.002479</td>\n",
       "      <td>-4130613151412272962</td>\n",
       "      <td>518379290858953980</td>\n",
       "      <td>4808654653650998822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.3673710119489219e+18</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>14596137</td>\n",
       "      <td>21579</td>\n",
       "      <td>33358308</td>\n",
       "      <td>0.122886</td>\n",
       "      <td>0.001092</td>\n",
       "      <td>0.106842</td>\n",
       "      <td>0.002479</td>\n",
       "      <td>-4130613151412272962</td>\n",
       "      <td>518379290858953980</td>\n",
       "      <td>4808654653650998822</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       id  click  hour_of_day  day_of_week  is_weekend  \\\n",
       "0    1.43198015250962e+19      0            8            2           0   \n",
       "1  1.4342335790673269e+19      0            7            2           0   \n",
       "2  1.5949798049261076e+19      0            7            2           0   \n",
       "3  6.2652196368133274e+18      0           10            2           0   \n",
       "4  7.3673710119489219e+18      0           10            2           0   \n",
       "\n",
       "   site_freq  app_freq  device_freq  te_site_id  te_app_id  te_device_model  \\\n",
       "0   14596137     21579     33358308    0.122886   0.001092         0.107136   \n",
       "1   14596137     21579     33358308    0.122886   0.001092         0.109834   \n",
       "2   14596137     21579     33358308    0.122886   0.001092         0.109834   \n",
       "3   14596137     21579     33358308    0.122886   0.001092         0.128226   \n",
       "4   14596137     21579     33358308    0.122886   0.001092         0.106842   \n",
       "\n",
       "     te_C14  hash_sitecat_devtype  hash_appcat_devtype         hash_c14_c17  \n",
       "0  0.002479  -4130613151412272962   518379290858953980  4808654653650998822  \n",
       "1  0.002479  -4130613151412272962   518379290858953980  4808654653650998822  \n",
       "2  0.002479  -4130613151412272962   518379290858953980  4808654653650998822  \n",
       "3  0.002479  -4130613151412272962   518379290858953980  4808654653650998822  \n",
       "4  0.002479  -4130613151412272962   518379290858953980  4808654653650998822  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fs = GcsFileSystem()\n",
    "\n",
    "# list parquet parts in the folder\n",
    "infos = fs.get_file_info(FileSelector(BUCKET_DIR, recursive=False))\n",
    "parts = sorted([i.path for i in infos if i.is_file and i.path.endswith(\".parquet\")])\n",
    "print(\"Parquet parts found:\", len(parts))\n",
    "assert parts, \"No parquet parts found. Re-check your export folder.\"\n",
    "\n",
    "# load a manageable subset of files + only the columns you need\n",
    "t0 = time.time()\n",
    "dataset_small = ds.dataset(parts[:N_FILES], filesystem=fs, format=\"parquet\")\n",
    "df = dataset_small.to_table(columns=COLUMNS).to_pandas()\n",
    "print(\"Loaded:\", df.shape, f\"in {time.time()-t0:.1f}s\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "acbb6a7f-35d4-4fbb-a722-a2aa1bc82a33",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15374036"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d78b8a1-ce79-4df6-a5b7-39e5b1496510",
   "metadata": {},
   "source": [
    "### prep features + split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8dff919-a1c8-4736-adda-b65b3f1ebc66",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/Test: (12299228, 13) (3074808, 13) | pos rate: 0.0986866817982397 0.09868648709122652\n"
     ]
    }
   ],
   "source": [
    "target = \"click\"\n",
    "X = df.drop(columns=[\"id\", target], errors=\"ignore\").apply(pd.to_numeric, errors=\"coerce\").fillna(0.0)\n",
    "y = df[target].astype(int)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.20, stratify=y, random_state=42\n",
    ")\n",
    "print(\"Train/Test:\", X_train.shape, X_test.shape, \"| pos rate:\", y_train.mean(), y_test.mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f9f900-0091-48e1-b779-ddab705055a0",
   "metadata": {},
   "source": [
    "### train XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6dac918-63fc-4116-b782-5cccb876fcdd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC     : 0.8348861356995055\n",
      "PR-AUC  : 0.3567079322644577\n",
      "LogLoss : 0.5102630613572855\n",
      "Accuracy: 0.7184871380587016\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import roc_auc_score, log_loss, accuracy_score, average_precision_score\n",
    "\n",
    "# make features compact\n",
    "X_train32 = X_train.astype(\"float32\")\n",
    "X_test32  = X_test.astype(\"float32\")\n",
    "\n",
    "pos = y_train.sum()\n",
    "neg = y_train.shape[0] - pos\n",
    "spw = float(neg) / max(float(pos), 1.0)\n",
    "\n",
    "model = xgb.XGBClassifier(\n",
    "    n_estimators=200,          # ↓ fewer trees\n",
    "    learning_rate=0.1,         # ↑ faster learning\n",
    "    max_depth=6,               # shallower trees\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    reg_lambda=1.0,\n",
    "    objective=\"binary:logistic\",\n",
    "    eval_metric=[\"logloss\",\"auc\",\"aucpr\"],\n",
    "    tree_method=\"hist\",\n",
    "    random_state=42,\n",
    "    scale_pos_weight=spw,\n",
    "    n_jobs=-1,                 # use all CPU cores\n",
    ")\n",
    "model.fit(X_train32, y_train, eval_set=[(X_test32, y_test)], verbose=False)\n",
    "\n",
    "proba = model.predict_proba(X_test32)[:, 1]\n",
    "pred  = (proba >= 0.5).astype(int)\n",
    "print(\"AUC     :\", roc_auc_score(y_test, proba))\n",
    "print(\"PR-AUC  :\", average_precision_score(y_test, proba))\n",
    "print(\"LogLoss :\", log_loss(y_test, proba))\n",
    "print(\"Accuracy:\", accuracy_score(y_test, pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5daba52c-f1cd-4399-a301-21b7b369288a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07d856e6-78f0-4175-a9a5-591ec2118feb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used shards: 30/1500 (~2.0%)\n",
      "Rows used in this run: 15,374,036\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import gcsfs\n",
    "from pyarrow.fs import GcsFileSystem, FileSelector\n",
    "\n",
    "BUCKET_DIR = \"avazu-ctr/processed/fe_v2_parquet\"  \n",
    "N_FILES = 30  \n",
    "\n",
    "# count shards in the folder\n",
    "fs = GcsFileSystem()\n",
    "infos = fs.get_file_info(FileSelector(BUCKET_DIR, recursive=False))\n",
    "total_parts = sum(1 for i in infos if i.is_file and i.path.endswith(\".parquet\"))\n",
    "\n",
    "used_parts = min(N_FILES, total_parts)\n",
    "rows_used = len(df)  # df is your training dataframe\n",
    "\n",
    "print(f\"Used shards: {used_parts}/{total_parts} (~{used_parts/total_parts:.1%})\")\n",
    "print(f\"Rows used in this run: {rows_used:,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f552fc94-b085-4de2-8f8d-5a232b81616e",
   "metadata": {},
   "source": [
    "## Phase 3 — Advanced Model (XGBoost with v2 Features)\n",
    "\n",
    "### Goal:\n",
    "Go beyond the baseline Logistic Regression by adding stronger features (target encoding + hashed interactions) and training an XGBoost classifier to improve ranking (AUC) and probability quality for CTR.\n",
    "\n",
    "### Data & Sampling\n",
    "#### Source table: avazu_processed.train_v2_sample (deterministic sample from the full dataset)\n",
    "\n",
    "Modeling format: exported to Parquet at gs://avazu-ctr/processed/fe_v2_parquet/\n",
    "\n",
    "This run (for fast iteration on current compute):\n",
    "\n",
    "Used shards: 30 / 1500 (~2.0% of Parquet parts)\n",
    "\n",
    "Rows used: 15,374,036\n",
    "\n",
    "We purposely trained on a small portion of the Parquet parts to keep the dev loop responsive on the current machine.\n",
    "\n",
    "### v2 Feature Set\n",
    "\n",
    "- Time: hour_of_day, day_of_week, is_weekend\n",
    "\n",
    "- Frequencies: site_freq, app_freq, device_freq\n",
    "\n",
    "- Target Encoding (leak-safe, smoothed): te_site_id, te_app_id, te_device_model, te_C14\n",
    "\n",
    "- Hashed Interactions: hash_sitecat_devtype, hash_appcat_devtype, hash_c14_c17\n",
    "\n",
    "### Model & Metrics \n",
    "\n",
    "- Model: XGBoost (tree_method=hist, imbalance via scale_pos_weight)\n",
    "\n",
    "- AUC: 0.835\n",
    "\n",
    "- PR-AUC: 0.357\n",
    "\n",
    "- LogLoss: 0.510\n",
    "\n",
    "- Accuracy: 0.718\n",
    "\n",
    "\n",
    "v2 features + XGBoost deliver a strong lift in ranking (AUC) and a meaningful drop in LogLoss. Great sign the engineered signals are useful.\n",
    "\n",
    "### Improving Probability Quality (next steps)\n",
    "\n",
    "Since the end-goal is better-calibrated click probabilities, here’s a simple path forward (no complexity, just practical steps):\n",
    "\n",
    "- Calibration: add Platt (sigmoid) or isotonic calibration on a held-out slice to reduce LogLoss and sharpen probabilities.\n",
    "\n",
    "- Early stopping + light tuning: use native xgb.train early stopping; sweep a few knobs (depth 5–7, min_child_weight 1–5, subsample/colsample 0.7–0.9, mild reg_alpha).\n",
    "\n",
    "- More data, gradually: bump the number of Parquet parts and keep early stopping on; or resize the VM.\n",
    "\n",
    "- Temporal validation: train on earlier dates, test on later dates to mimic production drift (more realistic lift estimates).\n",
    "\n",
    "- Feature refinements: try time-aware target encoding (per hour/day), a couple more safe interactions, and re-check for leakage.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d62551-3e62-44b1-b356-95b1caad6489",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ea51c930-ac4a-4f93-88d2-5d4cf9ba9a4a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded model: projects/87568676021/locations/us-central1/models/5156510522664812544\n",
      "Endpoint: projects/87568676021/locations/us-central1/endpoints/2995591942085017600\n",
      "Currently deployed: [('4978543570592989184', 'ctr_xgb_v2_named')]\n"
     ]
    }
   ],
   "source": [
    "# --- CELL 1: Upload new model; locate existing endpoint ---\n",
    "PROJECT = \"click-through-rate-prediction\"\n",
    "REGION  = \"us-central1\"\n",
    "BUCKET  = \"avazu-ctr\"\n",
    "\n",
    "# set this to the NEW folder you just uploaded\n",
    "ART_DIR = \"models/ctr_xgb/2025-10-08T06-28-20_named\"\n",
    "\n",
    "ENDPOINT_DISPLAY = \"ctr_xgb_endpoint\"  \n",
    "MODEL_DISPLAY    = \"ctr_xgb_v2_named\"   \n",
    "from google.cloud import aiplatform, storage\n",
    "import xgboost as xgb\n",
    "\n",
    "# pick serving image that matches your local xgboost\n",
    "ver = xgb.__version__.split(\".\")\n",
    "major, minor = int(ver[0]), int(ver[1])\n",
    "if   major >= 2 and minor >= 1:  SERVING_IMAGE = \"us-docker.pkg.dev/vertex-ai/prediction/xgboost-cpu.2-1:latest\"\n",
    "elif major >= 2:                 SERVING_IMAGE = \"us-docker.pkg.dev/vertex-ai/prediction/xgboost-cpu.2-0:latest\"\n",
    "else:                            SERVING_IMAGE = \"us-docker.pkg.dev/vertex-ai/prediction/xgboost-cpu.1-7:latest\"\n",
    "\n",
    "aiplatform.init(project=PROJECT, location=REGION)\n",
    "\n",
    "# sanity check artifacts exist\n",
    "bkt = storage.Client(project=PROJECT).bucket(BUCKET)\n",
    "assert bkt.blob(f\"{ART_DIR}/model.bst\").exists(), \"model.bst missing\"\n",
    "assert bkt.blob(f\"{ART_DIR}/feature_list.json\").exists(), \"feature_list.json missing\"\n",
    "\n",
    "# upload new model (creates a fresh model resource)\n",
    "model = aiplatform.Model.upload(\n",
    "    display_name=MODEL_DISPLAY,\n",
    "    artifact_uri=f\"gs://{BUCKET}/{ART_DIR}\",\n",
    "    serving_container_image_uri=SERVING_IMAGE,\n",
    "    labels={\"project\":\"ctr\",\"stage\":\"v2\",\"named\":\"true\"},\n",
    ")\n",
    "print(\"Uploaded model:\", model.resource_name)\n",
    "\n",
    "# find (or create once) the endpoint by display name\n",
    "eps = [e for e in aiplatform.Endpoint.list() if e.display_name == ENDPOINT_DISPLAY]\n",
    "endpoint = eps[0] if eps else aiplatform.Endpoint.create(display_name=ENDPOINT_DISPLAY)\n",
    "print(\"Endpoint:\", endpoint.resource_name)\n",
    "\n",
    "# show currently deployed models\n",
    "print(\"Currently deployed:\", [(m.id, m.display_name) for m in endpoint.list_models()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca89368-f81f-4e41-8906-7d86d467c9d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m133",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m133"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
