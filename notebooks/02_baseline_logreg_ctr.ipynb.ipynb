{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "140d94da-d809-496e-90c7-78f8d9b53ddd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.7228998731069742\n",
      "PR-AUC: 0.33750028120750164\n",
      "LogLoss: 0.5307046715801752\n",
      "Accuracy: 0.7122319055565857\n",
      "Positives in train/test: 0.1694588923192934 0.16945423079300329\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, log_loss, accuracy_score, average_precision_score\n",
    "\n",
    "# 1) Load your exported sample\n",
    "path = \"gs://avazu-ctr/processed/fe_v1_sample/part-000000000000.csv\"\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "# 2) Define columns\n",
    "drop_cols = [\"click\", \"id\", \"date\"]  # drop non-features + non-numeric date\n",
    "num_cols = [\"hour_of_day\", \"day_of_week\", \"is_weekend\", \"site_freq\", \"app_freq\", \"device_freq\"]\n",
    "\n",
    "# All remaining non-numeric columns (object dtype) are categorical\n",
    "candidate_feature_cols = [c for c in df.columns if c not in drop_cols]\n",
    "cat_cols = [c for c in candidate_feature_cols if df[c].dtype == \"object\"]\n",
    "# Numeric columns include those we listed explicitly (ensure they exist)\n",
    "num_cols = [c for c in num_cols if c in df.columns]\n",
    "\n",
    "# 3) Split\n",
    "X = df[cat_cols + num_cols]\n",
    "y = df[\"click\"].astype(int)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# 4) Preprocessing + Model\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols),\n",
    "        (\"num\", StandardScaler(with_mean=False), num_cols),\n",
    "    ],\n",
    "    sparse_threshold=1.0  # keep sparse\n",
    ")\n",
    "\n",
    "clf = LogisticRegression(\n",
    "    solver=\"saga\",            # good for large, sparse one-hots\n",
    "    max_iter=200,\n",
    "    class_weight=\"balanced\",\n",
    "    n_jobs=-1,\n",
    "    C=1.0,\n",
    "    penalty=\"l2\",\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "pipe = Pipeline([\n",
    "    (\"prep\", preprocess),\n",
    "    (\"model\", clf),\n",
    "])\n",
    "\n",
    "# 5) Train\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "# 6) Evaluate\n",
    "proba = pipe.predict_proba(X_test)[:, 1]\n",
    "pred = (proba >= 0.5).astype(int)\n",
    "\n",
    "print(\"AUC:\", roc_auc_score(y_test, proba))\n",
    "print(\"PR-AUC:\", average_precision_score(y_test, proba))\n",
    "print(\"LogLoss:\", log_loss(y_test, proba))\n",
    "print(\"Accuracy:\", accuracy_score(y_test, pred))\n",
    "\n",
    "# Optional: sanity checks\n",
    "print(\"Positives in train/test:\", y_train.mean(), y_test.mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e18443-f8fb-447d-9f36-4319091911e2",
   "metadata": {},
   "source": [
    "#  Phase 2 — Baseline Logistic Regression (CTR)\n",
    "\n",
    "**Data:** `gs://avazu-ctr/processed/fe_v1_sample/part-000000000000.csv`  \n",
    "**Target:** `click` (binary)  \n",
    "**Features used:**\n",
    "- **Time-based:** `hour_of_day`, `day_of_week`, `is_weekend`\n",
    "- **Count-based:** `site_freq`, `app_freq`, `device_freq`\n",
    "- **Categorical IDs:** site/app/device/app/site identifiers (one-hot encoded)\n",
    "- **Dropped:** `id`, `date`\n",
    "\n",
    "**Preprocessing pipeline:**\n",
    "- `OneHotEncoder(handle_unknown='ignore')` for categorical columns  \n",
    "- `StandardScaler(with_mean=False)` for numeric columns  \n",
    "- Combined with `ColumnTransformer` and trained in an `sklearn.Pipeline`\n",
    "\n",
    "**Model:** `LogisticRegression(solver='saga', class_weight='balanced', max_iter=200, C=1.0, random_state=42)`\n",
    "\n",
    "###  Metrics (Test Set)\n",
    "- **ROC AUC:** 0.7229  \n",
    "- **PR AUC:** 0.3375  \n",
    "- **LogLoss:** 0.5307  \n",
    "- **Accuracy:** 0.7122  \n",
    "- **Positive rate (train/test):** 0.1695 / 0.1695\n",
    "\n",
    "### Interpretation\n",
    "- AUC ~0.72 is a **solid baseline** for sparse CTR data with simple features.\n",
    "- PR-AUC ~0.34 reflects reasonable ranking ability given the ~17% positive rate.\n",
    "- LogLoss ~0.53 indicates moderate calibration; expect improvements with richer features (target encoding, interactions, embeddings) or stronger models (GBDT, deep CTR).\n",
    "\n",
    "### What’s next (Phase 2.5 / Phase 3)\n",
    "- Try **hashing trick** or **count/target encodings** to reduce dimensionality.\n",
    "- Compare against **XGBoost/LightGBM** and later **PyTorch embeddings** for high-cardinality categoricals.\n",
    "\n",
    "*Notes:* Keep this notebook as the **baseline reference** to compare future models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867defa5-6779-4ea1-8d9d-687147d4e3fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m133",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m133"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
